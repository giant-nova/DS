{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/Images/SN_logo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extracting Stock Data Using a Web Scraping</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all stock data is available via API in this assignment; you will use web-scraping to obtain financial data. You will be quizzed on your results.  \n",
    " Using beautiful soup we will extract historical share data from a web-page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li>Downloading the Webpage Using Requests Library</li>\n",
    "        <li>Parsing Webpage HTML Using BeautifulSoup</li>\n",
    "        <li>Extracting Data and Building DataFrame</li>\n",
    "    </ul>\n",
    "<p>\n",
    "    Estimated Time Needed: <strong>30 min</strong></p>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'mamba' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'mamba' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml==4.6.4\n",
      "  Using cached lxml-4.6.4.tar.gz (3.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: lxml\n",
      "  Building wheel for lxml (setup.py): started\n",
      "  Building wheel for lxml (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for lxml\n",
      "Failed to build lxml\n",
      "Installing collected packages: lxml\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 4.9.2\n",
      "    Uninstalling lxml-4.9.2:\n",
      "      Successfully uninstalled lxml-4.9.2\n",
      "  Running setup.py install for lxml: started\n",
      "  Running setup.py install for lxml: finished with status 'error'\n",
      "  Rolling back uninstall of lxml\n",
      "  Moving to d:\\jupyter\\envs\\pyproject\\lib\\site-packages\\lxml-4.9.2.dist-info\\\n",
      "   from D:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\~xml-4.9.2.dist-info\n",
      "  Moving to d:\\jupyter\\envs\\pyproject\\lib\\site-packages\\lxml\\\n",
      "   from D:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\~xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [74 lines of output]\n",
      "  Building lxml version 4.6.4.\n",
      "  Building without Cython.\n",
      "  Building against pre-built libxml2 andl libxslt libraries\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\builder.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\cssselect.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\doctestcompare.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\ElementInclude.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\pyclasslookup.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\sax.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\usedoctest.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\_elementpath.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\builder.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\clean.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\defs.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\diff.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\ElementSoup.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\formfill.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\html5parser.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\soupparser.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\usedoctest.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_diffcommand.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_html5builder.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_setmixin.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\n",
      "  copying src\\lxml\\isoschematron\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\n",
      "  copying src\\lxml\\etree.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\etree_api.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\lxml.etree.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\lxml.etree_api.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\includes\\c14n.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\config.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\dtdvalid.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etreepublic.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\htmlparser.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\relaxng.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\schematron.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\tree.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\uri.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xinclude.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlerror.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlparser.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlschema.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xpath.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xslt.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etree_defs.h -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\lxml-version.h -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\rng\n",
      "  copying src\\lxml\\isoschematron\\resources\\rng\\iso-schematron.rng -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\rng\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\RNG2Schtrn.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\XSD2Schtrn.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_abstract_expand.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_dsdl_include.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_message.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_skeleton_for_xslt1.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_svrl_for_xslt1.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\readme.txt -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  running build_ext\n",
      "  building 'lxml.etree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for lxml\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for lxml did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [76 lines of output]\n",
      "  Building lxml version 4.6.4.\n",
      "  Building without Cython.\n",
      "  Building against pre-built libxml2 andl libxslt libraries\n",
      "  running install\n",
      "  D:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\builder.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\cssselect.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\doctestcompare.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\ElementInclude.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\pyclasslookup.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\sax.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\usedoctest.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\_elementpath.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\builder.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\clean.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\defs.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\diff.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\ElementSoup.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\formfill.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\html5parser.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\soupparser.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\usedoctest.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_diffcommand.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_html5builder.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\_setmixin.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  copying src\\lxml\\html\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\html\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\n",
      "  copying src\\lxml\\isoschematron\\__init__.py -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\n",
      "  copying src\\lxml\\etree.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\etree_api.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\lxml.etree.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\lxml.etree_api.h -> build\\lib.win-amd64-cpython-311\\lxml\n",
      "  copying src\\lxml\\includes\\c14n.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\config.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\dtdvalid.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etreepublic.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\htmlparser.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\relaxng.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\schematron.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\tree.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\uri.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xinclude.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlerror.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlparser.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xmlschema.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xpath.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\xslt.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\__init__.pxd -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\etree_defs.h -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  copying src\\lxml\\includes\\lxml-version.h -> build\\lib.win-amd64-cpython-311\\lxml\\includes\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\rng\n",
      "  copying src\\lxml\\isoschematron\\resources\\rng\\iso-schematron.rng -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\rng\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\RNG2Schtrn.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\XSD2Schtrn.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\n",
      "  creating build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_abstract_expand.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_dsdl_include.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_message.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_skeleton_for_xslt1.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_svrl_for_xslt1.xsl -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  copying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\readme.txt -> build\\lib.win-amd64-cpython-311\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\n",
      "  running build_ext\n",
      "  building 'lxml.etree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "lxml\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas==1.3.3\n",
    "#!pip install requests==2.26.0\n",
    "!mamba install bs4==4.10.0 -y\n",
    "!mamba install html5lib==1.1 -y\n",
    "!pip install lxml==4.6.4\n",
    "#!pip install plotly==5.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\requests\\compat.py:12\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "File \u001b[1;32mD:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\requests\\__init__.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n",
      "File \u001b[1;32mD:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\requests\\exceptions.py:11\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mrequests.exceptions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m BaseHTTPError\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m CompatJSONDecodeError\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRequestException\u001b[39;00m(\u001b[38;5;167;01mIOError\u001b[39;00m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    request.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\requests\\compat.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Pythons\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Syntax sugar.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\charset_normalizer\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[1;32mD:\\Jupyter\\envs\\pyproject\\Lib\\site-packages\\charset_normalizer\\api.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Webscraping to Extract Stock Data Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must use the `request` library to downlaod the webpage, and extract the text. We will extract Netflix stock data [https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m data  \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html\"\n",
    "\n",
    "data  = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must parse the text into html using `beautiful_soup`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can turn the html table into a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
    "\n",
    "# First we isolate the body of the table which contains all the information\n",
    "# Then we loop through each row and find all the column values for each row\n",
    "\n",
    "for row in soup.find(\"tbody\").find_all('tr'):\n",
    "    col = row.find_all(\"td\")\n",
    "    date = col[0].text\n",
    "    Open = col[1].text\n",
    "    high = col[2].text\n",
    "    low = col[3].text\n",
    "    close = col[4].text\n",
    "    adj_close = col[5].text\n",
    "    volume = col[6].text\n",
    "    \n",
    "    # Finally we append the data of each row to the table\n",
    "    netflix_data = netflix_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj Close\":adj_close, \"Volume\":volume}, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print out the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the pandas `read_html` function using the url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_html_pandas_data = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can convert the BeautifulSoup object to a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_html_pandas_data = pd.read_html(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beacause there is only one table on the page, we just take the first table in the list returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_dataframe = read_html_pandas_data[0]\n",
    "\n",
    "netflix_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Webscraping to Extract Stock Data Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `requests` library to download the webpage [https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html). Save the text of the response as a variable named `html_data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html\"\n",
    "\n",
    "html_data= requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the html data using `beautiful_soup`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup= BeautifulSoup(html_data, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1</b> What is the content of the title attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using beautiful soup extract the table with historical share prices and store it into a dataframe named `amazon_data`. The dataframe should have columns Date, Open, High, Low, Close, Adj Close, and Volume. Fill in each variable with the correct data from the list `col`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"])\n",
    "\n",
    "for row in soup.find(\"tbody\").find_all(\"tr\"):\n",
    "    col = row.find_all(\"td\")\n",
    "    date = col[0].text\n",
    "    Open = col[1].text\n",
    "    high = col[2].text\n",
    "    low = col[3].text\n",
    "    close = col[4].text\n",
    "    adj_close = col[5].text\n",
    "    volume = col[6].text\n",
    "    \n",
    "    amazon_data = amazon_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj Close\":adj_close, \"Volume\":volume}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first five rows of the `amazon_data` dataframe you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2</b> What is the name of the columns of the dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3</b> What is the `Open` of the last row of the amazon_data dataframe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data.iloc[60,1]\n",
    "#amazon_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork900-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "Azim Hirjani\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By    | Change Description        |\n",
    "| ----------------- | ------- | ------------- | ------------------------- |\n",
    "    | 2021-06-09       | 1.2     | Lakshmi Holla|Added URL in question 3 |\n",
    "| 2020-11-10        | 1.1     | Malika Singla | Deleted the Optional part |\n",
    "| 2020-08-27        | 1.0     | Malika Singla | Added lab to GitLab       |\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n",
    "\n",
    "<p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
